using System;
using System.Collections.Generic;
using DotnetSpider.Core;
using DotnetSpider.Core.Pipeline;
using DotnetSpider.Core.Processor;
using DotnetSpider.Core.Scheduler;
using DotnetSpider.Core.Selector;
using DotnetSpider.Core.Downloader;
using System.Text;
using DotnetSpider.Extension.Pipeline;

namespace DotnetSpider.Sample
{
    public class BaseUsage
    {
        #region Custmize processor and pipeline 完全自定义页面解析和数据管道

        public static void CustmizeProcessorAndPipeline()
        {
            // Config encoding, header, cookie, proxy etc... 定义采集的 Site 对象, 设置 Header、Cookie、代理等
            var site = new Site { EncodingName = "UTF-8", RemoveOutboundLinks = true };
            for (int i = 1; i < 5; ++i)
            {
                // Add start/feed urls. 添加初始采集链接
                site.AddStartUrl($"http://list.youku.com/category/show/c_96_s_1_d_1_p_{i}.html");
            }
            Spider spider = Spider.Create(site,
                // use memoery queue scheduler. 使用内存调度
                new QueueDuplicateRemovedScheduler(),
                // use custmize processor for youku 为优酷自定义的 Processor
                new YoukuPageProcessor())
                // use custmize pipeline for youku 为优酷自定义的 Pipeline
                .AddPipeline(new YoukuPipeline());
            spider.Downloader = new HttpClientDownloader();
           
            spider.ThreadNum = 1;
            spider.EmptySleepTime = 3000;

            // Start crawler 启动爬虫
            spider.Run();
        }

        public class YoukuPipeline : DefaultMySqlPipeline
        {
            private static long count = 0;

            public override void Process(IList<ResultItems> resultItems, ISpider spider)
            {
                foreach (var resultItem in resultItems)
                {
                    StringBuilder builder = new StringBuilder();
                    foreach (YoukuVideo entry in resultItem.Results["VideoResult"])
                    {
                        count++;
                        builder.Append($" [YoukuVideo {count}] {entry.Name}");
                    }
                    Console.WriteLine(builder);
                }
                // Other actions like save data to DB. 可以自由实现插入数据库或保存到文件

                base.Process(resultItems, spider);
            }
        }

        public class YoukuPageProcessor : BasePageProcessor
        {
            protected override void Handle(Page page)
            {
                // 利用 Selectable 查询并构造自己想要的数据对象
                var totalVideoElements = page.Selectable.SelectList(Selectors.XPath("//div[@class='yk-pack pack-film']")).Nodes();
                List<YoukuVideo> results = new List<YoukuVideo>();
                foreach (var videoElement in totalVideoElements)
                {
                    var video = new YoukuVideo();
                    video.Name = videoElement.Select(Selectors.XPath(".//img[@class='quic']/@alt")).GetValue();
                    results.Add(video);
                }

                // Save data object by key. 以自定义KEY存入page对象中供Pipeline调用
                page.AddResultItem("VideoResult", results);

                // Add target requests to scheduler. 解析需要采集的URL
                foreach (var url in page.Selectable.SelectList(Selectors.XPath("//ul[@class='yk-pages']")).Links().Nodes())
                {
                    page.AddTargetRequest(new Request(url.GetValue(), null));
                }
            }
        }

        public class YoukuVideo
        {
            public string Name { get; set; }
        }

        #endregion

        #region Crawler pages without traverse 采集指定页面不做遍历
        /// <summary>
        /// 页面爬虫
        /// </summary>
        public static void CrawlerPagesWithoutTraverse()
        {
            var site = new Site { EncodingName = "UTF-8", RemoveOutboundLinks = true };
            for (int i = 1; i < 5; ++i)
            {
                site.AddStartUrl("http://" + $"www.youku.com/v_olist/c_97_g__a__sg__mt__lg__q__s_1_r_0_u_0_pt_0_av_0_ag_0_sg__pr__h__d_1_p_{i}.html");
            }

            Spider spider = Spider.Create(site,
                "YOUKU_" + DateTime.Now.ToString("yyyyMMddhhmmss"),
                new QueueDuplicateRemovedScheduler(),
                new SimplePageProcessor())
                .AddPipeline(new FilePipeline());
            spider.ThreadNum = 2;
            spider.EmptySleepTime = 3000;

            // 启动爬虫
            spider.Run();
        }

        #endregion

        #region Crawler pages traversal 遍历整站
        /// <summary>
        /// 页面爬虫深度遍历
        /// </summary>
        public static void CrawlerPagesTraversal()
        {
            // Config encoding, header, cookie, proxy etc... 定义采集的 Site 对象, 设置 Header、Cookie、代理等
            var site = new Site { EncodingName = "UTF-8", RemoveOutboundLinks = true };

            // Set start/seed url
            site.AddStartUrl("http://www.cnblogs.com/");

            Spider spider = Spider.Create(site,
                // crawler identity
                "cnblogs_" + DateTime.Now.ToString("yyyyMMddhhmmss"),
                // use memoery queue scheduler
                new QueueDuplicateRemovedScheduler(),
                // default page processor will save whole html, and extract urls to target urls via regex
                new DefaultPageProcessor(new[] { "cnblogs\\.com" }))
                // save crawler result to file in the folder: \{running directory}\data\{crawler identity}\{guid}.dsd
                .AddPipeline(new FilePipeline());
  
            // dowload html by http client
            spider.Downloader = new HttpClientDownloader();
            // 4 threads 4线程
            spider.ThreadNum = 1;
            // traversal deep 遍历深度
            spider.Deep = 2;

            // stop crawler if it can't get url from the scheduler after 30000 ms 当爬虫连续30秒无法从调度中心取得需要采集的链接时结束.
            spider.EmptySleepTime = 30000;

            // start crawler 启动爬虫
            spider.Run();
        }

        #endregion
    }
}